{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils\n",
    "\n",
    "import csv\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CUDA</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Q-learning Network</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, hidden_size=64):\n",
    "        \n",
    "        super(QNetwork, self).__init__()\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, action_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.layers(state)\n",
    "    \n",
    "    def act(self, state, epsilon=0):\n",
    "        \n",
    "        if training_mode == True:\n",
    "        \n",
    "            if random.random() > epsilon:\n",
    "\n",
    "                # gets the best action according to the policy being trained\n",
    "                action = self.act_optimaly(state)\n",
    "\n",
    "            else:\n",
    "                \n",
    "                # gets a random action\n",
    "                action = random.randrange(self.action_size)\n",
    "        else:\n",
    "            \n",
    "            # gets the best action according to the already trained policy\n",
    "            action = self.act_optimaly(state)\n",
    "                \n",
    "        return action\n",
    "    \n",
    "    def act_optimaly(self, state):\n",
    "        \n",
    "        # gets the best action indicated by the current policy\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            q_value = self.forward(state)\n",
    "        self.train()\n",
    "\n",
    "        action  = q_value.max(1)[1].item()\n",
    "        \n",
    "        return action\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Dueling Network</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingQNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, hidden_size=64):\n",
    "        \n",
    "        super(DuelingQNetwork, self).__init__()\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.initial_layers = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.advantage = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, action_size)\n",
    "        )\n",
    "        \n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, state):\n",
    "        \n",
    "        x = self.initial_layers(state)\n",
    "        \n",
    "        advantage = self.advantage(x)\n",
    "        \n",
    "        value = self.value(x)\n",
    "        \n",
    "        return value + advantage - advantage.mean()\n",
    "    \n",
    "    \n",
    "    def act(self, state, epsilon=0):\n",
    "        \n",
    "        if training_mode == True:\n",
    "        \n",
    "            if random.random() > epsilon:\n",
    "\n",
    "                # gets the best action according to the policy being trained\n",
    "                action = self.act_optimaly(state)\n",
    "\n",
    "            else:\n",
    "                \n",
    "                # gets a random action\n",
    "                action = random.randrange(self.action_size)\n",
    "        else:\n",
    "            \n",
    "            # gets the best action according to the already trained policy\n",
    "            action = self.act_optimaly(state)\n",
    "                \n",
    "        return action\n",
    "    \n",
    "    def act_optimaly(self, state):\n",
    "        \n",
    "        # gets the best action indicated by the current policy\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            q_value = self.forward(state)\n",
    "        self.train()\n",
    "\n",
    "        action  = q_value.max(1)[1].item()\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Custom Loss Functions</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMSELoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CustomMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, input_tensor, target_tensor):\n",
    "\n",
    "        # calculates the Mean Squared Error between the input and target tensors\n",
    "        batch_loss_tensor = (input_tensor - target_tensor) ** 2 \n",
    "        \n",
    "        # returns the total loss tensor\n",
    "        return batch_loss_tensor.sum()\n",
    "\n",
    "class CustomMSEWeightedLoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CustomMSEWeightedLoss, self).__init__()\n",
    "\n",
    "    def forward(self, input_tensor, target_tensor, weights_list):\n",
    "\n",
    "        # calculates the Mean Squared Error between the input and target tensors\n",
    "        batch_loss_tensor = (input_tensor - target_tensor) ** 2\n",
    "\n",
    "        # creates a tensor to hold the weights\n",
    "        weights_tensor = torch.autograd.Variable(torch.Tensor(weights_list))\n",
    "\n",
    "        # multiplies by the weights\n",
    "        weighted_batch_loss = weights_tensor * batch_loss_tensor\n",
    "\n",
    "        # gets the total loss\n",
    "        weighted_loss_tensor = weighted_batch_loss.sum()\n",
    "        \n",
    "        return weighted_loss_tensor\n",
    "\n",
    "    \n",
    "class CustomHuberLoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CustomHuberLoss, self).__init__()\n",
    "\n",
    "    def forward(self, input_tensor, target_tensor, threshold=0.5):\n",
    "        \n",
    "        # like mean-squared-error - MSE because the error is small, and # like mean-absolute-error - MAE when error is large\n",
    "        batch_loss_tensor = (torch.abs(input_tensor - target_tensor) < threshold).float() * (((input_tensor - target_tensor) ** 2) * threshold) + (torch.abs(input_tensor - target_tensor) >= threshold).float() * (torch.abs(input_tensor - target_tensor) - threshold)\n",
    "        \n",
    "        #print(\"batch_loss_tensor\", batch_loss_tensor)\n",
    "        \n",
    "        # returns the total loss tensor\n",
    "        return batch_loss_tensor.sum()\n",
    "\n",
    "class CustomWeightedHuberLoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CustomWeightedHuberLoss, self).__init__()\n",
    "\n",
    "    def forward(self, input_tensor, target_tensor, weights_list, threshold=0.5):\n",
    "\n",
    "        # like mean-squared-error - MSE because the error is small, and # like mean-absolute-error - MAE when error is large\n",
    "        batch_loss_tensor = (torch.abs(input_tensor - target_tensor) < threshold).float() * (((input_tensor - target_tensor) ** 2) * threshold) + (torch.abs(input_tensor - target_tensor) >= threshold).float() * (torch.abs(input_tensor - target_tensor) - threshold)\n",
    "        \n",
    "        # creates a tensor to hold the weights\n",
    "        weights_tensor = torch.autograd.Variable(torch.Tensor(weights_list))\n",
    "\n",
    "        # multiplies by the weights\n",
    "        weighted_batch_loss = weights_tensor * batch_loss_tensor\n",
    "\n",
    "        # gets the total loss\n",
    "        weighted_loss_tensor = weighted_batch_loss.sum()\n",
    "        #print(\"weighted_loss_tensor\", weighted_loss_tensor)\n",
    "        \n",
    "        return weighted_loss_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Replay Buffer</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n",
    "\n",
    "from segment_tree import SumSegmentTree, MinSegmentTree\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size, seed):\n",
    "        \n",
    "        self._storage = []\n",
    "        self._maxsize = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self._next_idx = 0\n",
    "\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "\n",
    "        e = (state, action, reward, next_state, done)\n",
    "\n",
    "        if self._next_idx >= len(self._storage):\n",
    "            self._storage.append(e)\n",
    "        else:\n",
    "            self._storage[self._next_idx] = e\n",
    "\n",
    "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
    "\n",
    "    def _encode_samples(self, indices):\n",
    "\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "\n",
    "        for i in indices:\n",
    "\n",
    "            data = self._storage[i]\n",
    "\n",
    "            state, action, reward, next_state, done = data\n",
    "\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "\n",
    "        return (np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones))\n",
    "\n",
    "    def sample(self):\n",
    "\n",
    "        indices = [random.randint(0, len(self._storage) - 1) for _ in range(self.batch_size)]\n",
    "\n",
    "        return self._encode_samples(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Prioritised Replay Buffer</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer(ReplayBuffer):\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size, seed, alpha, epsilon):\n",
    "\n",
    "        super(PrioritizedReplayBuffer, self).__init__(buffer_size, batch_size, seed)\n",
    "\n",
    "        assert alpha >= 0\n",
    "\n",
    "        self._alpha = alpha\n",
    "        self._epsilon = epsilon\n",
    "\n",
    "        it_capacity = 1\n",
    "\n",
    "        while it_capacity < buffer_size:\n",
    "            it_capacity *= 2\n",
    "\n",
    "        self._it_sum = SumSegmentTree(it_capacity)\n",
    "        self._it_min = MinSegmentTree(it_capacity)\n",
    "\n",
    "        self._max_priority = 1.0\n",
    "\n",
    "\n",
    "    def add(self, *args, **kwargs):\n",
    "\n",
    "        idx = self._next_idx\n",
    "\n",
    "        super().add(*args, **kwargs)\n",
    "\n",
    "        self._it_sum[idx] = (self._max_priority ** self._alpha) + self._epsilon\n",
    "        self._it_min[idx] = (self._max_priority ** self._alpha) + self._epsilon\n",
    "\n",
    "\n",
    "    def _sample_proportional(self):\n",
    "\n",
    "        res = []\n",
    "\n",
    "        for _ in range(self.batch_size):\n",
    "\n",
    "            mass = random.random() * self._it_sum.sum(0, len(self._storage) - 1)\n",
    "\n",
    "            idx = self._it_sum.find_prefixsum_idx(mass)\n",
    "\n",
    "            res.append(idx)\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "    def sample(self, beta):\n",
    "\n",
    "        # gets a sample of indices\n",
    "        indices = self._sample_proportional()\n",
    "\n",
    "        # retrieves the states, actions, rewards, next_states and dones for the sampled indices\n",
    "        states, actions, rewards, next_states, dones = self._encode_samples(indices)\n",
    "\n",
    "        # calculates new weights\n",
    "        normalised_is_weights = []\n",
    "\n",
    "        p_min = self._it_min.min() / self._it_sum.sum()\n",
    "\n",
    "        # gets the maximum weight for normalisation purposes\n",
    "        max_weight = (p_min * len(self._storage)) ** (-beta)\n",
    "\n",
    "        for idx in indices:\n",
    "            \n",
    "            # computes the sample probability\n",
    "            p_sample = self._it_sum[idx] / self._it_sum.sum()\n",
    "\n",
    "            # computes the new weight using the current annealed beta\n",
    "            weight = (p_sample * len(self._storage)) ** (-beta)\n",
    "\n",
    "            # normalises the weight to scale the update down\n",
    "            normalised_is_weights.append(weight / max_weight)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones, indices, normalised_is_weights)\n",
    "\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "\n",
    "            self._it_sum[idx] = (priority ** self._alpha) +  self._epsilon\n",
    "            self._it_min[idx] = (priority ** self._alpha) +  self._epsilon\n",
    "\n",
    "            self._max_priority = max(self._max_priority, priority)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>DQN Agent</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, replay_buffer_size=100000, batch_size=128, learning_rate=5e-4, seed=0, per_alpha=0.6, per_epsilon=0.01):\n",
    "\n",
    "        # states and action sizes\n",
    "        self.state_size = state_dim\n",
    "        self.action_size = action_dim\n",
    "        \n",
    "        # seed to be used in random calls\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        # instantiates a new replay buffer\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "        \n",
    "        # size of batch of experiments to use when training the agent's Q-network\n",
    "        self.batch_size  = batch_size\n",
    "        \n",
    "        # stores the hyper-parameters that control \n",
    "        self.per_alpha = per_alpha\n",
    "        self.per_epsilon = per_epsilon\n",
    "        \n",
    "        # instantiates a replay buffer\n",
    "        if use_prioritised_replay == True:\n",
    "            self.memory = PrioritizedReplayBuffer(self.replay_buffer_size, self.batch_size, self.seed, self.per_alpha, self.per_epsilon)\n",
    "        else:\n",
    "            self.memory = ReplayBuffer(self.replay_buffer_size, self.batch_size, self.seed)\n",
    "\n",
    "        # instantiates the principal and target Q-Networks\n",
    "        if use_duelling_networks == True:\n",
    "            self.qnetwork_local = DuelingQNetwork(self.state_size, self.action_size).to(device)\n",
    "            self.qnetwork_target = DuelingQNetwork(self.state_size, self.action_size).to(device)\n",
    "        else:\n",
    "            self.qnetwork_local = QNetwork(self.state_size, self.action_size).to(device)\n",
    "            self.qnetwork_target = QNetwork(self.state_size, self.action_size).to(device)\n",
    "        \n",
    "        # synchronises the networks\n",
    "        self.hard_update(self.qnetwork_local, self.qnetwork_target)\n",
    "        \n",
    "        # optimiser for the principal network neing trained\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr = learning_rate)\n",
    "        \n",
    "        # number of steps that the agent performs before learning\n",
    "        self.steps_to_learning = 1\n",
    "        \n",
    "        # number of steps to take before copying the local networks' parameters to the target networks\n",
    "        self.target_network_parameter_update_steps = 20\n",
    "        \n",
    "    \n",
    "    def act(self, state, epsilon=0):\n",
    "\n",
    "        # gets an action from the current policy\n",
    "        return self.qnetwork_local.act(state, epsilon)\n",
    "        \n",
    "    \n",
    "    def step(self, step, state, action, reward, next_state, done, per_beta):\n",
    "        \n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # learns from experiences in the replay buffer\n",
    "        self.learn(step, per_beta)\n",
    "\n",
    "\n",
    "    def learn(self, step, per_beta, gamma=0.99, tau=1e-3):\n",
    "\n",
    "        # if the replay buffer has enough experiences then we can start learning from them in mini-batches\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            \n",
    "            # only learn every n steps: stability\n",
    "            if step % self.steps_to_learning == 0:\n",
    "\n",
    "                # collects a mini-batch of experiences from the replay buffer\n",
    "                if use_prioritised_replay == True:\n",
    "                    experiences = self.memory.sample(per_beta)\n",
    "                    states, actions, rewards, next_states, dones, indices, normalised_is_weights = experiences\n",
    "                else:\n",
    "                    experiences = self.memory.sample()\n",
    "                    states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "                # converts the numpy arrays to torch tensors\n",
    "                states_tensor      = torch.FloatTensor(np.float32(states)).to(device)\n",
    "                actions_tensor     = torch.LongTensor(actions).to(device)\n",
    "                rewards_tensor     = torch.FloatTensor(np.float32(rewards)).to(device)\n",
    "                next_states_tensor = torch.FloatTensor(np.float32(next_states)).to(device)\n",
    "                dones_tensor       = torch.FloatTensor(np.float32(dones)).to(device)\n",
    "                \n",
    "                # gets the state-action values for the current state (64x4) matrix\n",
    "                online_current_q_values = self.qnetwork_local.forward(states_tensor)\n",
    "                \n",
    "                # gets the state-action values of the selected actions for the current state (64x1) matrix\n",
    "                online_predicted_q_value = online_current_q_values.gather(1, actions_tensor.unsqueeze(1)).squeeze(1)\n",
    "                \n",
    "                # gets the state-action values for the next state (64x4) matrix\n",
    "                online_next_q_values = self.qnetwork_local.forward(next_states_tensor)\n",
    "                \n",
    "                # ---------------------------------------------------------------------------------------------------\n",
    "                \n",
    "                # gets the Q-values for the next state and for all actions.\n",
    "                # this will be used for the target estimations and for the Double Q-learning evaluation step\n",
    "                \n",
    "                # gets the state-action values for the next state from the target network (64x4) matrix\n",
    "                target_next_q_values = self.qnetwork_target.forward(next_states_tensor)\n",
    "                \n",
    "                # ---------------------------------------------------------------------------------------------------\n",
    "                \n",
    "                if use_double_q_learning == True:\n",
    "                \n",
    "                    # action selection step in Double Q-learning\n",
    "                    #\n",
    "                    # gets the maximum state-action values and the corresponding maximising action\n",
    "                    maximum_next_q_values_and_actions = torch.max(online_next_q_values, 1)\n",
    "\n",
    "                    # gets the maximising actions only\n",
    "                    maximum_actions = maximum_next_q_values_and_actions[1]\n",
    "                    \n",
    "                    # adds an extra dimension to the actions tensor\n",
    "                    maximum_actions_unsqueezed = maximum_actions.unsqueeze(1)\n",
    "                    \n",
    "                    # action evaluation step in Double Q-learning\n",
    "                    #\n",
    "                    # gets the state-action values of the selected maximising actions (64x1)\n",
    "                    target_next_q_value = target_next_q_values.gather(1, maximum_actions_unsqueezed).squeeze(1)\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    # gets the state-action values of the maximising actions (64x1)\n",
    "                    target_next_q_value = target_next_q_values.max(1)[0]\n",
    "                \n",
    "                # ---------------------------------------------------------------------------------------------------\n",
    "                \n",
    "                # calculates the expected state-action values using the Bellman equation\n",
    "                expected_q_value = rewards_tensor + gamma * target_next_q_value * (1 - dones_tensor)\n",
    "                \n",
    "                # ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "                # compares the expected and obtained state-action values\n",
    "                if use_huber_loss == True:\n",
    "                    \n",
    "                    if use_prioritised_replay == True:\n",
    "                        loss_fn = CustomWeightedHuberLoss()\n",
    "                        loss = loss_fn(online_predicted_q_value, expected_q_value, normalised_is_weights)\n",
    "                    else:\n",
    "                        loss_fn = CustomHuberLoss()\n",
    "                        loss = loss_fn(online_predicted_q_value, expected_q_value)\n",
    "                else:\n",
    "                    \n",
    "                    if use_prioritised_replay == True:\n",
    "                        loss_fn = CustomMSEWeightedLoss()\n",
    "                        loss = loss_fn(online_predicted_q_value, expected_q_value, normalised_is_weights)\n",
    "                    else:\n",
    "                        loss_fn = CustomMSELoss()\n",
    "                        loss = loss_fn(online_predicted_q_value, expected_q_value)\n",
    "                \n",
    "                # ---------------------------------------------------------------------------------------------------\n",
    "                \n",
    "                if use_prioritised_replay == True:\n",
    "                    \n",
    "                    # gets the td-errors as a numpy array for the replay buffer update\n",
    "                    td_errors = abs(expected_q_value - online_predicted_q_value).detach().numpy()\n",
    "\n",
    "                    # update the batch experiences priorities according to the td-errors\n",
    "                    priorities = np.power(np.abs(td_errors) + self.per_epsilon, self.per_alpha)\n",
    "\n",
    "                    # updates the sampled experiences priorities\n",
    "                    self.memory.update_priorities(indices, priorities)\n",
    "\n",
    "                # ---------------------------------------------------------------------------------------------------\n",
    "                \n",
    "                # clears the gradients\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # performs a backward pass on the network\n",
    "                loss.backward()\n",
    "                \n",
    "                # normalises the gradients of the principal network\n",
    "                if clip_gradients == True:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.qnetwork_local.parameters(), 1)\n",
    "\n",
    "                # performs a parameter update based on the current gradients and the chosen update rule (whe are using adam)\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # updates the target network\n",
    "                if step % self.target_network_parameter_update_steps == 0:\n",
    "                    \n",
    "                    # copies all paramters from the principal to the target network\n",
    "                    self.hard_update(self.qnetwork_local, self.qnetwork_target)\n",
    "                    \n",
    "\n",
    "    def hard_update(self, local_model, target_model):\n",
    "        target_model.load_state_dict(local_model.state_dict())\n",
    "\n",
    "        \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "\n",
    "    def save_agent_model(self, model_path):\n",
    "\n",
    "        # saves the actor model parameters for later usage in test or production\n",
    "        torch.save(self.qnetwork_local.state_dict(), model_path)\n",
    "\n",
    "\n",
    "    def load_agent_model(self, model_path):\n",
    "        \n",
    "        # loads a saved model for inference purposes\n",
    "        self.qnetwork_local.load_state_dict(torch.load(model_path))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Auxiliary Functions</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(i_episode, scores):\n",
    "    clear_output(True)\n",
    "\n",
    "    plt.figure(figsize=(30,5))\n",
    "\n",
    "    plt.subplot(131)\n",
    "\n",
    "    plt.title('Episode %s - Last episode average score: %s' % (i_episode, scores[-1]))\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.plot(np.arange(1, len(scores) + 1), scores)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11b408320>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "episodes = 1700\n",
    "\n",
    "get_episode_epsilon = lambda episode_idx, eps_greedy_start, eps_greedy_final, eps_greedy_decay: eps_greedy_final + (eps_greedy_start - eps_greedy_final) * math.exp(-1. * episode_idx / eps_greedy_decay)\n",
    "\n",
    "epsilon_greedy_decay_schedule = [get_episode_epsilon(i, 1.0, 0.01, (episodes / 20)) for i in range(episodes)]\n",
    "\n",
    "plt.plot(epsilon_greedy_decay_schedule)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training Routine</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(number_of_episodes=1700, max_steps_per_episode=1000, average_score_episodes=100, print_every_episodes=10, print_every_steps=1000, eps_greedy_start=1.0, eps_greedy_end=0.01, per_beta0=0.4):\n",
    "\n",
    "    # scores bookkeeping\n",
    "    last_100_episodes_scores = deque(maxlen = average_score_episodes)\n",
    "    scores_log = []\n",
    "    \n",
    "    # instantiates the agent\n",
    "    agent = Agent(state_size, action_size)\n",
    "\n",
    "    # starts counter in seconds\n",
    "    time_start = time.time()\n",
    "\n",
    "    # calculates our epsilon-greedy decay rate\n",
    "    epsilon_greedy_decay_rate = (number_of_episodes / 20)\n",
    "    \n",
    "    # builds our epsilon-greedy exploration decay schedule\n",
    "    epsilon_greedy_decay_schedule = [get_episode_epsilon(i, eps_greedy_start, eps_greedy_end, epsilon_greedy_decay_rate) for i in range(number_of_episodes)]\n",
    "    \n",
    "    for i_episode in range(1, number_of_episodes + 1):\n",
    "\n",
    "        # calculates a new prioritised experience replay beta to be used for importance sampling bias correction\n",
    "        per_beta = per_beta0 + i_episode * (1 - per_beta0) / number_of_episodes\n",
    "        \n",
    "        # gets a new epsilon_greedy for this episode\n",
    "        epsilon_greedy = epsilon_greedy_decay_schedule[i_episode - 1]\n",
    "        \n",
    "        # resets the environment\n",
    "        env_info = env.reset(train_mode = True)[brain_name]\n",
    "\n",
    "        # gets the current state of the environment (for each agent)\n",
    "        state = env_info.vector_observations[0]\n",
    "\n",
    "        # initializes the episode's total score\n",
    "        episode_total_score = 0\n",
    "\n",
    "        for step in range(1, max_steps_per_episode + 1):\n",
    "\n",
    "            # gets an action from the current policy using an epsilon-greed exploratory strategy\n",
    "            action = agent.act(state, epsilon_greedy)\n",
    "\n",
    "            # takes a step in the environment\n",
    "            env_info = env.step(action)[brain_name]\n",
    "\n",
    "            # gets reward, next_state and done\n",
    "            reward = env_info.rewards[0]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            done = env_info.local_done[0]\n",
    "                \n",
    "            # takes a step in the agent\n",
    "            agent.step(step, state, action, reward, next_state, done, per_beta)\n",
    "\n",
    "            # sets the next state\n",
    "            state = next_state\n",
    "\n",
    "            # updates the current episode's score\n",
    "            episode_total_score += reward\n",
    "            \n",
    "            # echoes the episode's current total reward\n",
    "            if step % print_every_steps == 0:\n",
    "                print(\"\\nEpisode {} step {} total reward so far -> {}\".format(i_episode, step, episode_total_score))\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        # adds this episode average score to the complete scores log\n",
    "        scores_log.append(episode_total_score)\n",
    "        \n",
    "        # adds this episode's total score to the last 100 scores average deque\n",
    "        last_100_episodes_scores.append(episode_total_score)\n",
    "        \n",
    "        # plots the latest 100 episodes average reward\n",
    "        if i_episode % print_every_episodes == 0:\n",
    "            plot_scores(i_episode, scores_log)\n",
    "            print(\"\\nEpisode {} average score so far -> {}\".format(i_episode, np.mean(last_100_episodes_scores)))\n",
    "            \n",
    "        # checks if the environment is solved\n",
    "        if np.mean(last_100_episodes_scores) > 13.0:\n",
    "            break\n",
    "\n",
    "    # plots the final scores \n",
    "    plot_scores(i_episode, scores_log)\n",
    "    print(\"\\nEpisode {} average score so far -> {}\".format(i_episode, np.mean(last_100_episodes_scores)))\n",
    "            \n",
    "    # plots the episode that resolved the task\n",
    "    print(\"Environment resolved in {} episodes!\".format(i_episode))\n",
    "    \n",
    "    # stop the timer\n",
    "    time_end = time.time()\n",
    "\n",
    "    # prints the duration in minutes\n",
    "    print(\"Environment resolved in {0:.2f} minutes!\".format((time_end-time_start) / 60))\n",
    "    \n",
    "    # gets the current date/time\n",
    "    now = datetime.datetime.now()\n",
    "        \n",
    "    # builds a new model name\n",
    "    model_path = 'agent_model_trained_' + now.strftime(\"%Y-%m-%d_%H%M%S\") + '.pth'\n",
    "    \n",
    "    # saves the learned model\n",
    "    agent.save_agent_model(model_path)\n",
    "\n",
    "    # echoes model path\n",
    "    print(\"Model saved at -> \", model_path)\n",
    "    \n",
    "    # closes the environment\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm settings\n",
    "use_huber_loss = True\n",
    "clip_gradients = True\n",
    "use_double_q_learning = True\n",
    "use_prioritised_replay = True\n",
    "use_duelling_networks = True\n",
    "\n",
    "training_mode = False\n",
    "\n",
    "if training_mode == True:\n",
    "    \n",
    "    # echoes current task\n",
    "    print(\"Training started... :-)\")\n",
    "    \n",
    "    # instantiates a new environment in test mode\n",
    "    env = UnityEnvironment(file_name='Banana.app', no_graphics=True, seed=0)\n",
    "\n",
    "    # get the default brain\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "\n",
    "    # reset the environment\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "    # number of agents\n",
    "    num_agents = len(env_info.agents)\n",
    "    print('Number of agents:', num_agents)\n",
    "\n",
    "    # size of each action\n",
    "    action_size = brain.vector_action_space_size\n",
    "    print('Size of each action:', action_size)\n",
    "\n",
    "    # examine the state space\n",
    "    state = env_info.vector_observations[0]\n",
    "    state_size = state.shape[0]\n",
    "\n",
    "    print('There are {} agents. Each observes a state with length: {}'.format(num_agents, state_size))\n",
    "    print('The state for the first agent looks like:', state)\n",
    "    \n",
    "    # starts our tests\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Testing Routine</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(number_of_episodes=10, max_steps_per_episode=200, print_every_episodes=1, print_every_steps=10):\n",
    "\n",
    "    # scores bookkeeping\n",
    "    scores_log = []\n",
    "    \n",
    "    # instantiates the agent\n",
    "    agent = Agent(state_size, action_size)\n",
    "    \n",
    "    # loads a trained model\n",
    "    agent.load_agent_model(model_file_name)\n",
    "        \n",
    "    # starts counter in seconds\n",
    "    time_start = time.time()\n",
    "    \n",
    "    for i_episode in range(1, number_of_episodes + 1):\n",
    "\n",
    "        # resets the environment\n",
    "        env_info = env.reset(train_mode = False)[brain_name]\n",
    "\n",
    "        # gets the current state of the environment (for each agent)\n",
    "        state = env_info.vector_observations[0]\n",
    "\n",
    "        # initializes the episode's total score\n",
    "        episode_total_score = 0\n",
    "\n",
    "        for step in range(1, max_steps_per_episode + 1):\n",
    "\n",
    "            # gets an action from the current policy using an epsilon-greed exploratory strategy\n",
    "            action = agent.act(state)\n",
    "\n",
    "            # takes a step in the environment\n",
    "            env_info = env.step(action)[brain_name]\n",
    "\n",
    "            # gets reward, next_state and done\n",
    "            reward = env_info.rewards[0]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            done = env_info.local_done[0]\n",
    "                \n",
    "            # sets the next state\n",
    "            state = next_state\n",
    "\n",
    "            # updates the current episode's score\n",
    "            episode_total_score += reward\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        # adds this episode average score to the complete scores log\n",
    "        scores_log.append(episode_total_score)\n",
    "        \n",
    "        if i_episode % print_every_episodes == 0:\n",
    "            plot_scores(i_episode, scores_log)\n",
    "            print(\"\\nEpisode {} total score so far -> {}\".format(i_episode, np.sum(scores_log)))\n",
    "            \n",
    "\n",
    "    # stop the timer\n",
    "    time_end = time.time()\n",
    "\n",
    "    # plots the final scores obtained\n",
    "    plot_scores(i_episode, scores_log)\n",
    "    \n",
    "    # echoes model file name\n",
    "    print(\"Model used -> \", model_file_name)\n",
    "\n",
    "    # prints the duration in minutes\n",
    "    print(\"\\nEnvironment tested in {} minutes!\".format((time_end-time_start) / 60))\n",
    "    \n",
    "    # echoes the final score obtained by the trained model\n",
    "    print(\"\\nModel total score obtained -> {}\".format(np.sum(scores_log)))\n",
    "        \n",
    "    # closes the environment\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used ->  13-agent_model_trained_2018-11-10_230850.pth\n",
      "\n",
      "Environment tested in 3.395366382598877 minutes!\n",
      "\n",
      "Model total score obtained -> 106.0\n"
     ]
    }
   ],
   "source": [
    "model_file_name = \"13-agent_model_trained_2018-11-10_230850.pth\"\n",
    "\n",
    "use_prioritised_replay = True\n",
    "use_duelling_networks = True\n",
    "\n",
    "test_mode = True\n",
    "\n",
    "if test_mode == True:\n",
    "    \n",
    "    # echoes current task\n",
    "    print(\"Testing started... :-)\")\n",
    "    \n",
    "    # instantiates a new environment in test mode\n",
    "    env = UnityEnvironment(file_name='Banana.app', no_graphics=True, seed=1)\n",
    "\n",
    "    # get the default brain\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "\n",
    "    # reset the environment\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "\n",
    "    # number of agents\n",
    "    num_agents = len(env_info.agents)\n",
    "\n",
    "    # size of each action\n",
    "    action_size = brain.vector_action_space_size\n",
    "\n",
    "    # examine the state space\n",
    "    state = env_info.vector_observations[0]\n",
    "    state_size = state.shape[0]\n",
    "    \n",
    "    # starts our tests\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
